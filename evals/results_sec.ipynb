{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import Levenshtein\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sentence_transformers.util import cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/with_features_with_embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data/with_features_with_embeddings\")\n",
    "\n",
    "rows_1, rows_3 = [], []\n",
    "\n",
    "for model_folder in DATA_DIR.iterdir():\n",
    "    if not model_folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    for json_file in model_folder.glob(\"*.jsonl\"):\n",
    "        fname = json_file.name\n",
    "\n",
    "        if \"_1_\" in fname:\n",
    "            target_list = rows_1\n",
    "        elif \"_3_\" in fname:\n",
    "            target_list = rows_3\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        context = \"exp2_\" in fname\n",
    "        prev_num = None\n",
    "        if \"_3_\" in fname:\n",
    "            if \"context1\" in fname:\n",
    "                prev_num = 1\n",
    "            elif \"context3\" in fname:\n",
    "                prev_num = 3\n",
    "\n",
    "        with json_file.open(\"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    row_data = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON error in {json_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                ordered_row = {\n",
    "                    \"model_name\": model_folder.name,\n",
    "                    \"context\": context,\n",
    "                }\n",
    "                if prev_num is not None:\n",
    "                    ordered_row[\"prev_num\"] = prev_num\n",
    "                ordered_row.update(row_data)\n",
    "                target_list.append(ordered_row)\n",
    "\n",
    "# Note: files with \"_3_\" in the name correspond to what is referred to as Experiment 2 (exp2) in the paper.\n",
    "# For consistency with the paper, we assign them to exp2 here.\n",
    "\n",
    "exp1 = pd.DataFrame(rows_1)\n",
    "exp2 = pd.DataFrame(rows_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([exp1, exp2], ignore_index=True)\n",
    "\n",
    "unprocessed_pairs = all_df[all_df[\"is_processed\"] == False][[\"student_id\", \"question_name\"]].drop_duplicates()\n",
    "print(\"unique {student, question} pair with is_processed = false:\", len(unprocessed_pairs))\n",
    "\n",
    "matching_rows = all_df.merge(unprocessed_pairs, on=[\"student_id\", \"question_name\"])\n",
    "print(\"rows corresponding to these pairs:\", len(matching_rows))\n",
    "\n",
    "print(\"portion of data losing:\", len(matching_rows) / len(all_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unprocessed(df):\n",
    "    return df.merge(unprocessed_pairs, on=[\"student_id\", \"question_name\"], how=\"left\", indicator=True) \\\n",
    "             .query('_merge == \"left_only\"') \\\n",
    "             .drop(columns=[\"_merge\"])\n",
    "\n",
    "exp1_ = drop_unprocessed(exp1)\n",
    "exp2_ = drop_unprocessed(exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cleaning is specific to our dataset\n",
    "exp1_ = exp1_[~exp1_[\"question_name\"].isin([\"Mint\", \"accumulate\"])]\n",
    "exp2_ = exp2_[~exp2_[\"question_name\"].isin([\"Mint\", \"accumulate\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFlattener:\n",
    "    def __init__(self, is_flat=False):\n",
    "        self.is_flat = is_flat\n",
    "\n",
    "    def flatten_features(self, df):\n",
    "        df = df.copy()\n",
    "        if self.is_flat:\n",
    "            for q in range(3):\n",
    "                for code_type in [\"gt\", \"synthetic\"]:\n",
    "                    col = f\"{code_type}_code_block_q{q}_features\"\n",
    "                    if col in df.columns:\n",
    "                        features_df = pd.json_normalize(df[col]).set_index(df.index)\n",
    "                        prefix = f\"{code_type}_q{q}\"\n",
    "                        features_df.columns = [f\"{prefix}_{sub}\" for sub in features_df.columns]\n",
    "                        df = pd.concat([df.drop(columns=[col]), features_df], axis=1)\n",
    "        else:\n",
    "            for col, prefix in [(\"gt_code_block_features\", \"gt\"), (\"synthetic_code_block_features\", \"synthetic\")]:\n",
    "                if col in df.columns:\n",
    "                    features_df = pd.json_normalize(df[col]).set_index(df.index)\n",
    "                    features_df.columns = [f\"{prefix}_{sub}\" for sub in features_df.columns]\n",
    "                    df = pd.concat([df.drop(columns=[col]), features_df], axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def flatten_autograder(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        def normalize_error_type(raw):\n",
    "            if not isinstance(raw, str):\n",
    "                return None\n",
    "            if \"No Error\" in raw:\n",
    "                return \"No Error\"\n",
    "            elif \"Logical Error\" in raw:\n",
    "                return \"Logical Error\"\n",
    "            elif \"Runtime Error\" in raw or \"NameError\" in raw:\n",
    "                return \"Runtime Error\"\n",
    "            elif \"Compilation Error\" in raw:\n",
    "                return \"Compilation Error\"\n",
    "            return \"Other\"\n",
    "\n",
    "        if self.is_flat:\n",
    "            for q in range(3):\n",
    "                for code_type in [\"gt\", \"synthetic\"]:\n",
    "                    col = f\"{code_type}_code_block_q{q}_autograder\"\n",
    "                    df[f\"{code_type}_q{q}_error_type\"] = df[col].apply(\n",
    "                        lambda x: normalize_error_type(x.get(\"error_type\")) if isinstance(x, dict) else None\n",
    "                    )\n",
    "                    df[f\"{code_type}_q{q}_test_pass_rate\"] = df[col].apply(\n",
    "                        lambda x: x.get(\"test_pass_rate\") if isinstance(x, dict) else None\n",
    "                    )\n",
    "        else:\n",
    "            for code_type in [\"gt\", \"synthetic\"]:\n",
    "                col = f\"{code_type}_code_block_autograder\"\n",
    "                df[f\"{code_type}_error_type\"] = df[col].apply(\n",
    "                    lambda x: normalize_error_type(x.get(\"error_type\")) if isinstance(x, dict) else None\n",
    "                )\n",
    "                df[f\"{code_type}_test_pass_rate\"] = df[col].apply(\n",
    "                    lambda x: x.get(\"test_pass_rate\") if isinstance(x, dict) else None\n",
    "                )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def flatten_embeddings(self, df):\n",
    "        df = df.copy()\n",
    "        if self.is_flat:\n",
    "            for q in range(3):\n",
    "                for code_type in [\"gt\", \"synthetic\"]:\n",
    "                    col = f\"embeddings\"\n",
    "                    target = f\"{code_type}_code_block_q{q}\"\n",
    "                    df[f\"{target}_embedding\"] = df[col].apply(\n",
    "                        lambda x: x.get(target) if isinstance(x, dict) else None\n",
    "                    )\n",
    "        else:\n",
    "            df[\"gt_code_block_embedding\"] = df[\"embeddings\"].apply(lambda x: x.get(\"gt_code_block\") if isinstance(x, dict) else None)\n",
    "            df[\"synthetic_code_block_embedding\"] = df[\"embeddings\"].apply(lambda x: x.get(\"synthetic_code_block\") if isinstance(x, dict) else None)\n",
    "        return df\n",
    "\n",
    "flat = DataFlattener()\n",
    "exp1_clean = flat.flatten_features(exp1_)\n",
    "exp1_clean = flat.flatten_autograder(exp1_clean)\n",
    "exp1_clean = flat.flatten_embeddings(exp1_clean)\n",
    "\n",
    "\n",
    "flat = DataFlattener()\n",
    "exp2_clean = flat.flatten_features(exp2_)\n",
    "exp2_clean = flat.flatten_autograder(exp2_clean)\n",
    "exp2_clean = flat.flatten_embeddings(exp2_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_original_columns(df, is_flat=False):\n",
    "    cols_to_drop = [\"input\", \"output_synthetic\", \"output_gt\", \"embeddings\"]\n",
    "\n",
    "    if is_flat:\n",
    "        for q in range(3):\n",
    "            cols_to_drop += [\n",
    "                f\"gt_code_block_q{q}_autograder\",\n",
    "                f\"synthetic_code_block_q{q}_autograder\",\n",
    "            ]\n",
    "    else:\n",
    "        cols_to_drop += [\n",
    "            \"synthetic_code_block_autograder\",\n",
    "            \"gt_code_block_autograder\",\n",
    "        ]\n",
    "\n",
    "    return df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "\n",
    "exp1_clean = drop_original_columns(exp1_clean, is_flat=False)\n",
    "exp2_clean = drop_original_columns(exp2_clean, is_flat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_w292_violations(df, is_flat=False):\n",
    "    df = df.copy()\n",
    "\n",
    "    def has_w292(messages):\n",
    "        if isinstance(messages, list):\n",
    "            return any(isinstance(m, dict) and m.get(\"msg\") == \"W292 no newline at end of file\" for m in messages)\n",
    "        return False\n",
    "\n",
    "    if is_flat:\n",
    "        for q in range(3):\n",
    "            for code_type in [\"gt\", \"synthetic\"]:\n",
    "                msg_col = f\"{code_type}_code_block_q{q}_features_pep8_violations.messages\"\n",
    "                count_col = f\"{code_type}_code_block_q{q}_features_pep8_violations.count\"\n",
    "                if msg_col in df.columns and count_col in df.columns:\n",
    "                    df[count_col] = df.apply(\n",
    "                        lambda row: row[count_col] - 1 if has_w292(row.get(msg_col)) else row[count_col],\n",
    "                        axis=1\n",
    "                    )\n",
    "    else:\n",
    "        for code_type in [\"gt\", \"synthetic\"]:\n",
    "            msg_col = f\"{code_type}_pep8_violations.messages\"\n",
    "            count_col = f\"{code_type}_pep8_violations.count\"\n",
    "            if msg_col in df.columns and count_col in df.columns:\n",
    "                df[count_col] = df.apply(\n",
    "                    lambda row: row[count_col] - 1 if has_w292(row.get(msg_col)) else row[count_col],\n",
    "                    axis=1\n",
    "                )\n",
    "    return df\n",
    "\n",
    "exp1_clean = adjust_w292_violations(exp1_clean, is_flat=False)\n",
    "exp2_clean = adjust_w292_violations(exp2_clean, is_flat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_clean.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_clean.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_clean.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_metrics(df, exp_type=\"exp1\", question_name=None, test_class=None, group_by_quantile=False):\n",
    "    metric_keys = [\n",
    "        \"loc\", \"char_count\", \"ast_depth\", \"ast_width\",\n",
    "        \"ast_node_count\", \"pep8_violations.count\",\n",
    "        \"test_pass_rate\"\n",
    "    ]\n",
    "\n",
    "    # style_score will be computed in the later cells \n",
    "    \n",
    "    # metric_keys = [\n",
    "    #     \"loc\", \"char_count\", \"ast_depth\", \"ast_width\",\n",
    "    #     \"ast_node_count\", \"pep8_violations.count\",\n",
    "    #     \"test_pass_rate\", \"style_score\"\n",
    "    # ]\n",
    "\n",
    "    gt_cols = [f\"gt_{k}\" for k in metric_keys]\n",
    "    syn_cols = [f\"synthetic_{k}\" for k in metric_keys]\n",
    "\n",
    "    if question_name:\n",
    "        df = df[df[\"question_name\"] == question_name]\n",
    "    if test_class:\n",
    "        df = df[df[\"test_class\"] == test_class]\n",
    "\n",
    "    if exp_type == \"exp1\":\n",
    "        if group_by_quantile:\n",
    "            group_cols = [\"model_name\", \"quantile\", \"context\"]\n",
    "            gt_group_cols = [\"quantile\"]\n",
    "        else:\n",
    "            group_cols = [\"model_name\"]\n",
    "            gt_group_cols = []\n",
    "\n",
    "    elif exp_type == \"exp2\":\n",
    "        group_cols = [\"model_name\"]\n",
    "        gt_group_cols = []\n",
    "    else:\n",
    "        raise ValueError(\"exp_type must be 'exp1' or 'exp2'\")\n",
    "\n",
    "    syn_summary = df.groupby(group_cols)[syn_cols].agg(['mean', 'std']).reset_index()\n",
    "    syn_summary.columns = ['_'.join(col).replace('synthetic_', '').strip('_') for col in syn_summary.columns]\n",
    "\n",
    "    if gt_group_cols:\n",
    "        gt_summary = df.groupby(gt_group_cols)[gt_cols].agg(['mean', 'std']).reset_index()\n",
    "        gt_summary.columns = [\n",
    "            col[0].replace(\"gt_\", \"\") + (\"_\" + col[1] if col[1] else \"\")\n",
    "            for col in gt_summary.columns\n",
    "        ]\n",
    "        gt_summary[\"model_name\"] = \"GT\"\n",
    "        if \"quantile\" not in gt_summary.columns and \"quantile\" in syn_summary.columns:\n",
    "            gt_summary[\"quantile\"] = None\n",
    "    else:\n",
    "        gt_means = df[gt_cols].mean()\n",
    "        gt_stds = df[gt_cols].std()\n",
    "        gt_row = {f\"{col.replace('gt_', '')}_mean\": gt_means[col] for col in gt_cols}\n",
    "        gt_row.update({f\"{col.replace('gt_', '')}_std\": gt_stds[col] for col in gt_cols})\n",
    "        gt_row[\"model_name\"] = \"GT\"\n",
    "        if \"quantile\" in syn_summary.columns:\n",
    "            gt_row[\"quantile\"] = None\n",
    "        gt_summary = pd.DataFrame([gt_row])\n",
    "\n",
    "\n",
    "    for col in syn_summary.columns:\n",
    "        if col not in gt_summary.columns:\n",
    "            gt_summary[col] = None\n",
    "    gt_summary = gt_summary[syn_summary.columns]\n",
    "\n",
    "    summary = pd.concat([syn_summary, gt_summary], ignore_index=True)\n",
    "    summary = summary.round(3)\n",
    "\n",
    "    allowed_models = [\"gpt_4_1\", \"qwen_2_5_coder_7b\", \"qwen_2_5_coder_7b_inst\", \"GT\"]\n",
    "    # allowed_models = [\"qwen_3_8b\", \"llama_3_8b\", \"qwen_2_5_coder_3b\", \"GT\"]\n",
    "    summary = summary[summary[\"model_name\"].isin(allowed_models)]\n",
    "\n",
    "    selected_metrics = [\"test_pass_rate\",\"pep8_violations.count\"]\n",
    "    selected_cols = []\n",
    "    for metric in selected_metrics:\n",
    "        selected_cols += [f\"{metric}_mean\", f\"{metric}_std\"]\n",
    "    id_cols = [col for col in summary.columns if col in [\"model_name\", \"quantile\", \"context\"]]\n",
    "    summary = summary[id_cols + selected_cols]\n",
    "\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_metrics(exp1_clean, exp_type=\"exp1\", question_name=\"two_list\", test_class=\"test1\", group_by_quantile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_metrics(exp2_clean, exp_type=\"exp2\", question_name=\"two_list\", test_class=\"test1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_colors = {\n",
    "    \"Student\": \"#4d4d4d\",                 \n",
    "    \"gpt-4.1\": \"#8E5491\",            \n",
    "    \"qwen-student\": \"#D17F27\",   \n",
    "    \"qwen-inst\": \"#4F9058\", \n",
    "    \"qwen-2.5-coder-3b\": \"#BA35B5\",  \n",
    "    \"qwen-3-8b\": \"#ABDC96\",          \n",
    "    \"llama-3-8b\": \"#4D57B0\"           \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def prepare_error_df(df, model_name_col=\"model_name\"):\n",
    "    gt_df = df.drop_duplicates(subset=[\"student_id\", \"question_name\", \"quantile\"]).copy()\n",
    "    gt_df[\"model_source\"] = \"GT\"\n",
    "    gt_df[\"error_type\"] = gt_df[\"gt_error_type\"]\n",
    "\n",
    "    gt_true = gt_df.copy()\n",
    "    gt_true[\"context\"] = True\n",
    "    gt_false = gt_df.copy()\n",
    "    gt_false[\"context\"] = False\n",
    "    gt_df = pd.concat([gt_true, gt_false], ignore_index=True)\n",
    "\n",
    "    syn_df = df.copy()\n",
    "    syn_df[\"model_source\"] = syn_df[model_name_col]\n",
    "    syn_df[\"error_type\"] = syn_df[\"synthetic_error_type\"]\n",
    "\n",
    "    return pd.concat([gt_df, syn_df], ignore_index=True)\n",
    "\n",
    "\n",
    "def plot_error_distribution(df, title_prefix, question_name=None, include_models=None):\n",
    "    expected_errors = [\"No Error\", \"Logical Error\", \"Runtime Error\"]\n",
    "    df = df[df[\"context\"].notna()].copy()\n",
    "\n",
    "    if question_name:\n",
    "        df = df[df[\"question_name\"] == question_name]\n",
    "\n",
    "    if include_models is not None:\n",
    "        df = df[df[\"model_source\"].isin(include_models + [\"GT\"])]\n",
    "\n",
    "    df[\"error_type\"] = pd.Categorical(df[\"error_type\"], categories=expected_errors)\n",
    "\n",
    "    for test_class in sorted(df[\"test_class\"].dropna().unique()):\n",
    "        subset = df[df[\"test_class\"] == test_class].copy()\n",
    "\n",
    "        group_cols = [\"context\", \"quantile\", \"model_source\", \"error_type\"]\n",
    "        count_df = (\n",
    "            subset.groupby(group_cols)\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "\n",
    "        from itertools import product\n",
    "        contexts = [True, False]\n",
    "        quantiles = [\"submission_q0\", \"submission_q1\", \"submission_q2\"]\n",
    "        model_sources = subset[\"model_source\"].unique()\n",
    "        full_index = pd.DataFrame(product(contexts, quantiles, model_sources, expected_errors), columns=group_cols)\n",
    "        count_df = full_index.merge(count_df, how=\"left\").fillna({\"count\": 0})\n",
    "        count_df[\"count\"] = count_df[\"count\"].astype(int)\n",
    "\n",
    "        count_df[\"proportion\"] = count_df.groupby([\"context\", \"quantile\", \"model_source\"])[\"count\"].transform(lambda x: x / x.sum())\n",
    "\n",
    "        count_df[\"std\"] = np.sqrt(\n",
    "            count_df[\"proportion\"] * (1 - count_df[\"proportion\"]) /\n",
    "            count_df.groupby([\"context\", \"quantile\", \"model_source\"])[\"count\"].transform(\"sum\")\n",
    "        )\n",
    "\n",
    "        count_df[\"model_source\"] = count_df[\"model_source\"].replace({\n",
    "            \"qwen_2_5_coder_7b\":       \"qwen-student\",\n",
    "            \"qwen_2_5_coder_7b_inst\":  \"qwen-inst\",\n",
    "            \"gpt_4_1\":                 \"gpt-4.1\",\n",
    "            \"qwen_2_5_coder_3b\": \"qwen-2.5-coder-3b\",\n",
    "            \"qwen_3_8b\": \"qwen-3-8b\",\n",
    "            \"llama_3_8b\": \"llama-3-8b\",\n",
    "            \"GT\":                 \"Student\"\n",
    "            })\n",
    "  \n",
    "\n",
    "        bin_rename = {\n",
    "            \"submission_q0\": \"start\",\n",
    "            \"submission_q1\": \"middle\",\n",
    "            \"submission_q2\": \"last\"\n",
    "        }\n",
    "        count_df[\"quantile\"] = count_df[\"quantile\"].replace(bin_rename)\n",
    "\n",
    "        count_df[\"quantile\"] = pd.Categorical(count_df[\"quantile\"], categories=[\"start\", \"middle\", \"last\"], ordered=True)\n",
    "        count_df[\"context_label\"] = count_df[\"context\"].map({False: \"context=False\", True: \"context=True\"})\n",
    "\n",
    "\n",
    "\n",
    "        q_title = f\"{title_prefix} — {test_class}\" + (f\" — {question_name}\" if question_name else \"\")\n",
    "        print(q_title)  \n",
    "\n",
    "\n",
    "        fig = px.bar(\n",
    "            count_df,\n",
    "            x=\"error_type\",\n",
    "            template=\"plotly_white\",\n",
    "            y=\"proportion\",\n",
    "            color=\"model_source\",\n",
    "            color_discrete_map=model_colors,\n",
    "            barmode=\"group\",\n",
    "            facet_row=\"context_label\",\n",
    "            facet_col=\"quantile\",\n",
    "            category_orders={\n",
    "            \"error_type\": expected_errors,\n",
    "            \"context_label\": [\"context=False\", \"context=True\"],\n",
    "            \"quantile\": [\"start\", \"middle\", \"last\"]\n",
    "        },\n",
    "\n",
    "            title=None,\n",
    "            labels={\"proportion\": \"Proportion\", \"error_type\": \"Error Type\", \"quantile\": \"Bin\"},\n",
    "        )\n",
    "        \n",
    "        fig.for_each_annotation(\n",
    "            lambda a: a.update(text=a.text.replace(\"context_label=\", \"\").replace(\"quantile=\", \"\").replace(\"Bin=\", \"\"))\n",
    "        )\n",
    "\n",
    "        for context_value in [False, True]:\n",
    "            sub_df = count_df[count_df[\"context\"] == context_value].copy()\n",
    "            context_str = \"context=True\" if context_value else \"context=False\"\n",
    "\n",
    "            fig = px.bar(\n",
    "                sub_df,\n",
    "                x=\"error_type\",\n",
    "                template=\"plotly_white\",\n",
    "                y=\"proportion\",\n",
    "                color=\"model_source\",\n",
    "                error_y=\"std\",\n",
    "                color_discrete_map=model_colors,\n",
    "                barmode=\"group\",\n",
    "                facet_col=\"quantile\",\n",
    "                category_orders={\n",
    "                    \"error_type\": expected_errors,\n",
    "                    \"quantile\": [\"start\", \"middle\", \"last\"]\n",
    "                },\n",
    "                labels={\"proportion\": \"Proportion\", \"error_type\": \"Error Type\", \"quantile\": \"Bin\"},\n",
    "            )\n",
    "\n",
    "            fig.for_each_annotation(\n",
    "                lambda a: a.update(text=a.text.replace(\"quantile=\", \"\").replace(\"Bin=\", \"\"))\n",
    "            )\n",
    "\n",
    "            fig.for_each_annotation(lambda a: a.update(font=dict(size=22)))\n",
    "            fig.update_traces(error_y=dict(thickness=1, width=5, color=\"black\"))\n",
    "\n",
    "\n",
    "\n",
    "            fig.update_layout(\n",
    "                height=500,\n",
    "                width=1000,\n",
    "                font=dict(size=18, color=\"black\"),\n",
    "                legend_title_text=None,\n",
    "                legend=dict(\n",
    "                    font=dict(size=22),\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.7,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5,\n",
    "                    bordercolor=\"grey\",\n",
    "                    borderwidth=1,\n",
    "                )\n",
    "            )\n",
    "            fig.update_xaxes(\n",
    "                tickfont=dict(size=16, color='black'),\n",
    "                showgrid=False,\n",
    "                gridcolor='lightgray',\n",
    "                linecolor='black',\n",
    "                linewidth=1.0\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                tickfont=dict(size=16, color='black'),\n",
    "                showgrid=True,\n",
    "                gridcolor='lightgray',\n",
    "                linecolor='black',\n",
    "                linewidth=1.0\n",
    "            )\n",
    "\n",
    "            print(f\"{q_title} — {context_str}\")\n",
    "            # fig.write_image(f\"./plots/err-dist/app/{question_name}_{test_class}_{context_str}_err_dist.pdf\")\n",
    "            fig.show()\n",
    "\n",
    "\n",
    "\n",
    "exp1_errors = prepare_error_df(exp1_clean)\n",
    "plot_error_distribution(\n",
    "    exp1_errors,\n",
    "    \"exp1\",\n",
    "    question_name=\"two_list\",\n",
    "    include_models=[\"qwen_2_5_coder_7b\", \"qwen_2_5_coder_7b_inst\", \"gpt_4_1\"]\n",
    "    # include_models=[\"qwen_2_5_coder_3b\", \"qwen_3_8b\", \"llama_3_8b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_block_num(df, block_col=\"block_num\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    stream_max = (\n",
    "        df.groupby([\"student_id\", \"question_name\", \"model_name\", \"prev_num\", \"test_class\", \"context\"])[block_col]\n",
    "        .max()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    median_map = (\n",
    "        stream_max.groupby([\"question_name\", \"test_class\"])[block_col]\n",
    "        .median()\n",
    "        .round()\n",
    "        .astype(int)\n",
    "        .reset_index()\n",
    "        .rename(columns={block_col: \"global_max_block\"})\n",
    "    )\n",
    "\n",
    "    df = df.merge(median_map, on=[\"question_name\", \"test_class\"], how=\"left\")\n",
    "\n",
    "    df = df.sort_values(by=[\"student_id\", \"question_name\", \"model_name\", \"prev_num\",\"context\", block_col])\n",
    "\n",
    "    def normalize(group):\n",
    "        T = group[\"global_max_block\"].iloc[0]\n",
    "        n = len(group)\n",
    "        if n == 1:\n",
    "            return pd.Series([1] * n, index=group.index)\n",
    "        if n == T:\n",
    "            return pd.Series(range(1, T + 1), index=group.index)\n",
    "        if n < T:\n",
    "            norm_idx = np.linspace(1, T, n).round().astype(int)\n",
    "            return pd.Series(norm_idx, index=group.index)\n",
    "        bin_edges = np.linspace(0, n, T + 1).astype(int)\n",
    "        norm_idx = np.zeros(n, dtype=int)\n",
    "        for i in range(T):\n",
    "            norm_idx[bin_edges[i]:bin_edges[i+1]] = i + 1\n",
    "        return pd.Series(norm_idx, index=group.index)\n",
    "    \n",
    "    df[\"norm_block_num\"] = (\n",
    "        df.groupby([\"student_id\", \"question_name\", \"model_name\", \"prev_num\", \"context\"])\n",
    "        .apply(normalize)\n",
    "        .reset_index(level=[0, 1, 2, 3, 4], drop=True)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "exp2_clean = add_normalized_block_num(exp2_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_passrate_df(df, x_col, is_exp2=False):\n",
    "    df = df[df[\"context\"].notna()].copy()\n",
    "\n",
    "    if is_exp2 and \"prev_num\" in df.columns:\n",
    "        df[\"prev\"] = df[\"prev_num\"].astype(str)\n",
    "\n",
    "    gt_df = df.drop_duplicates(subset=[\"student_id\", \"question_name\", x_col]).copy()\n",
    "    gt_df[\"model_name\"] = \"Student\"\n",
    "    gt_df[\"test_pass_rate\"] = gt_df[\"gt_test_pass_rate\"]\n",
    "    gt_df[\"context\"] = False\n",
    "    gt_df2 = gt_df.copy()\n",
    "    gt_df2[\"context\"] = True\n",
    "    gt_df = pd.concat([gt_df, gt_df2], ignore_index=True)\n",
    "\n",
    "    syn_df = df.copy()\n",
    "    syn_df[\"test_pass_rate\"] = syn_df[\"synthetic_test_pass_rate\"]\n",
    "\n",
    "    combined = pd.concat([gt_df, syn_df], ignore_index=True)\n",
    "\n",
    "    if is_exp2:\n",
    "        combined[x_col] = combined[x_col].astype(int)\n",
    "    else:\n",
    "        if x_col == \"quantile\":\n",
    "            quantile_map = {\n",
    "                \"submission_q0\": \"start\",\n",
    "                \"submission_q1\": \"middle\",\n",
    "                \"submission_q2\": \"last\"\n",
    "            }\n",
    "            combined[x_col] = combined[x_col].map(quantile_map)\n",
    "            combined[x_col] = pd.Categorical(\n",
    "                combined[x_col],\n",
    "                categories=[\"start\", \"middle\", \"last\"],\n",
    "                ordered=True\n",
    "            )\n",
    "        else:\n",
    "            combined[x_col] = combined[x_col].astype(int)\n",
    "    combined[\"model_name\"] = combined[\"model_name\"].replace({\n",
    "        \"qwen_2_5_coder_7b\": \"qwen-student\",\n",
    "        \"qwen_2_5_coder_7b_inst\": \"qwen-inst\",\n",
    "        \"gpt_4_1\": \"gpt-4.1\",\n",
    "    })\n",
    "\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def plot_avg_passrate_progress_multi(df_list, x_col, is_exp2=False, question_name=None, include_models=None):\n",
    "    combined_all = []\n",
    "    for df, label in df_list:\n",
    "        df_prepped = prepare_passrate_df(df, x_col, is_exp2).copy()\n",
    "        if is_exp2 and \"prev_num\" in df.columns:\n",
    "            df_prepped[\"prev_num\"] = df_prepped[\"prev\"]\n",
    "        else:\n",
    "            df_prepped[\"prev_num\"] = \"All\"\n",
    "        combined_all.append(df_prepped)\n",
    "\n",
    "    combined_df = pd.concat(combined_all, ignore_index=True)\n",
    "\n",
    "    if question_name:\n",
    "        combined_df = combined_df[combined_df[\"question_name\"] == question_name]\n",
    "\n",
    "    if include_models is not None:\n",
    "        combined_df = combined_df[\n",
    "            (combined_df[\"model_name\"] == \"Student\")|\n",
    "            (combined_df[\"model_name\"].isin(include_models))\n",
    "        ]\n",
    "\n",
    "    for test_class in sorted(combined_df[\"test_class\"].dropna().unique()):\n",
    "        print(f\"== {question_name} — {test_class} — Avg Pass Rate Progress ==\")\n",
    "        df_tc = combined_df[combined_df[\"test_class\"] == test_class].copy()\n",
    "\n",
    "        if is_exp2:\n",
    "            bin_size = 1\n",
    "            df_tc[\"binned_step\"] = (df_tc[x_col] // bin_size) * bin_size\n",
    "            x_group = \"binned_step\"\n",
    "        else:\n",
    "            x_group = x_col\n",
    "\n",
    "        grouped = (\n",
    "            df_tc.groupby([x_group, \"model_name\", \"context\", \"prev_num\"])[\"test_pass_rate\"]\n",
    "            .agg([\"mean\", \"std\", \"count\"])\n",
    "            .reset_index()\n",
    "        )\n",
    "        grouped[\"sem\"] = grouped[\"std\"] / grouped[\"count\"]**0.5\n",
    "        grouped[\"ci95\"] = 1.96 * grouped[\"sem\"]\n",
    "        grouped[\"ci95\"] = grouped[\"ci95\"]\n",
    "\n",
    "        for context_value in [False, True]:\n",
    "            df_context = grouped[grouped[\"context\"] == context_value].copy()\n",
    "            context_str = \"context=True\" if context_value else \"context=False\"\n",
    "\n",
    "            fig = px.line(\n",
    "                df_context,\n",
    "                line_shape=\"spline\",\n",
    "                x=x_group,\n",
    "                template=\"plotly_white\",\n",
    "                y=\"mean\",\n",
    "                error_y=\"ci95\",\n",
    "                color=\"model_name\",\n",
    "                color_discrete_map=model_colors,\n",
    "                category_orders={\"model_name\": [\"Student\", \"qwen-student\", \"qwen-inst\", \"gpt-4.1\"]},\n",
    "                facet_col=\"prev_num\" if is_exp2 else None,\n",
    "                markers=True,\n",
    "                title=None,\n",
    "                labels={\"mean\": \"Avg. Test Pass Rate\", x_group: \"Submission Step\"},\n",
    "            )\n",
    "\n",
    "            for trace in fig.data:\n",
    "                trace.line.width = 2.5\n",
    "\n",
    "            fig.update_layout(\n",
    "                height=400,\n",
    "                width=1000,\n",
    "                font=dict(size=18, color='black'),\n",
    "                legend_title_text=None,\n",
    "                legend=dict(\n",
    "                    font=dict(size=22),\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.7,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5,\n",
    "                    bordercolor=\"grey\",\n",
    "                    borderwidth=1,\n",
    "                ),\n",
    "            )\n",
    "            fig.update_yaxes(matches=None)\n",
    "            fig.update_xaxes(tickmode=\"array\", tickvals=[2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "            fig.update_yaxes(tickmode=\"array\", tickvals=[0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "            fig.update_xaxes(\n",
    "                tickfont=dict(size=16, color='black'),\n",
    "                showgrid=True,\n",
    "                gridcolor='lightgray',\n",
    "                linecolor='black',\n",
    "                linewidth=1.0\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                tickfont=dict(size=16, color='black'),\n",
    "                showgrid=True,\n",
    "                gridcolor='lightgray',\n",
    "                linecolor='black',\n",
    "                linewidth=1.0\n",
    ")\n",
    "\n",
    "            print(f\"{question_name} — {test_class} — {context_str}\")\n",
    "            # fig.write_image(f\"plots/pass-rate/app/{question_name}_{test_class}_{context_str}_passrate.pdf\")\n",
    "            fig.show()\n",
    "\n",
    "plot_avg_passrate_progress_multi(\n",
    "    [\n",
    "        (exp2_clean[exp2_clean[\"prev_num\"] == 1], \"exp2 prev=1\"),\n",
    "        (exp2_clean[exp2_clean[\"prev_num\"] == 3], \"exp2 prev=3\")\n",
    "    ],\n",
    "    x_col=\"norm_block_num\",\n",
    "    is_exp2=True,\n",
    "    question_name=\"two_list\",\n",
    "    include_models=[\"qwen-student\", \"qwen-inst\", \"gpt-4.1\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_keys = [\n",
    "    \"loc\", \"char_count\",\n",
    "    \"ast_depth\", \"ast_width\", \"ast_node_count\"\n",
    "]\n",
    "\n",
    "gt_cols = [f\"gt_{k}\" for k in style_keys]\n",
    "syn_cols = [f\"synthetic_{k}\" for k in style_keys]\n",
    "\n",
    "exp1_style_df = exp1_clean[gt_cols + syn_cols].dropna()\n",
    "exp2_style_df = exp2_clean[gt_cols + syn_cols].dropna()\n",
    "\n",
    "combined_df = pd.concat([exp1_style_df, exp2_style_df], ignore_index=True)\n",
    "\n",
    "combined_style_values = pd.concat([\n",
    "    combined_df[gt_cols].rename(columns=lambda x: x.replace(\"gt_\", \"\")),\n",
    "    combined_df[syn_cols].rename(columns=lambda x: x.replace(\"synthetic_\", \"\"))\n",
    "], ignore_index=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(combined_style_values)\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "style_scores = pca.fit_transform(scaled_data).flatten()\n",
    "\n",
    "combined_df[\"gt_style_score\"] = style_scores[:len(combined_df)]\n",
    "combined_df[\"synthetic_style_score\"] = style_scores[len(combined_df):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1_weights = pd.Series(pca.components_[0], index=combined_style_values.columns)\n",
    "print(pc1_weights.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_clean = exp1_clean.copy()\n",
    "exp2_clean = exp2_clean.copy()\n",
    "\n",
    "exp1_clean[\"gt_style_score\"] = combined_df[\"gt_style_score\"].iloc[:len(exp1_clean)].values\n",
    "exp1_clean[\"synthetic_style_score\"] = combined_df[\"synthetic_style_score\"].iloc[:len(exp1_clean)].values\n",
    "\n",
    "exp2_clean[\"gt_style_score\"] = combined_df[\"gt_style_score\"].iloc[len(exp1_clean):].values\n",
    "exp2_clean[\"synthetic_style_score\"] = combined_df[\"synthetic_style_score\"].iloc[len(exp1_clean):].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_style_df(df, x_col, is_exp2=False):\n",
    "    df = df[df[\"context\"].notna()].copy()\n",
    "\n",
    "    if is_exp2 and \"prev_num\" in df.columns:\n",
    "        df[\"prev\"] = df[\"prev_num\"].astype(str)\n",
    "\n",
    "    gt_df = df.drop_duplicates(subset=[\"student_id\", \"question_name\", x_col]).copy()\n",
    "    gt_df[\"model_name\"] = \"Student\"\n",
    "    gt_df[\"style_score\"] = gt_df[\"gt_style_score\"]\n",
    "    gt_df[\"context\"] = False\n",
    "    gt_df2 = gt_df.copy()\n",
    "    gt_df2[\"context\"] = True\n",
    "    gt_df = pd.concat([gt_df, gt_df2], ignore_index=True)\n",
    "\n",
    "    syn_df = df.copy()\n",
    "    syn_df[\"style_score\"] = syn_df[\"synthetic_style_score\"]\n",
    "\n",
    "    combined = pd.concat([gt_df, syn_df], ignore_index=True)\n",
    "\n",
    "    if is_exp2:\n",
    "        combined[x_col] = combined[x_col].astype(int)\n",
    "    else:\n",
    "        if x_col == \"quantile\":\n",
    "            quantile_map = {\n",
    "                \"submission_q0\": \"start\",\n",
    "                \"submission_q1\": \"middle\",\n",
    "                \"submission_q2\": \"last\"\n",
    "            }\n",
    "            combined[x_col] = combined[x_col].map(quantile_map)\n",
    "            combined[x_col] = pd.Categorical(\n",
    "                combined[x_col],\n",
    "                categories=[\"start\", \"middle\", \"last\"],\n",
    "                ordered=True\n",
    "            )\n",
    "        else:\n",
    "            combined[x_col] = combined[x_col].astype(int)\n",
    "    combined[\"model_name\"] = combined[\"model_name\"].replace({\n",
    "        \"qwen_2_5_coder_7b\": \"qwen-student\",\n",
    "        \"qwen_2_5_coder_7b_inst\": \"qwen-inst\",\n",
    "        \"gpt_4_1\": \"gpt-4.1\",\n",
    "    })\n",
    "\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def plot_avg_style_progress_multi(df_list, x_col, is_exp2=False, question_name=None, include_models=None):\n",
    "    combined_all = []\n",
    "    for df, label in df_list:\n",
    "        df_prepped = prepare_style_df(df, x_col, is_exp2).copy()\n",
    "        if is_exp2 and \"prev_num\" in df.columns:\n",
    "            df_prepped[\"prev_num\"] = df_prepped[\"prev\"]\n",
    "        else:\n",
    "            df_prepped[\"prev_num\"] = \"All\"\n",
    "        combined_all.append(df_prepped)\n",
    "\n",
    "    combined_df = pd.concat(combined_all, ignore_index=True)\n",
    "\n",
    "    if question_name:\n",
    "        combined_df = combined_df[combined_df[\"question_name\"] == question_name]\n",
    "\n",
    "    if include_models is not None:\n",
    "        combined_df = combined_df[\n",
    "            (combined_df[\"model_name\"] == \"Student\")|\n",
    "            (combined_df[\"model_name\"].isin(include_models))\n",
    "        ]\n",
    "\n",
    "    for test_class in sorted(combined_df[\"test_class\"].dropna().unique()):\n",
    "        print(f\"== {question_name} — {test_class} — Avg Pass Rate Progress ==\")\n",
    "        df_tc = combined_df[combined_df[\"test_class\"] == test_class].copy()\n",
    "\n",
    "        if is_exp2:\n",
    "            bin_size = 1\n",
    "            df_tc[\"binned_step\"] = (df_tc[x_col] // bin_size) * bin_size\n",
    "            x_group = \"binned_step\"\n",
    "        else:\n",
    "            x_group = x_col\n",
    "\n",
    "        grouped = (\n",
    "            df_tc.groupby([x_group, \"model_name\", \"context\", \"prev_num\"])[\"style_score\"]\n",
    "            .agg([\"mean\", \"std\", \"count\"])\n",
    "            .reset_index()\n",
    "        )\n",
    "        grouped[\"sem\"] = grouped[\"std\"] / grouped[\"count\"]**0.5\n",
    "        grouped[\"ci95\"] = 1.96 * grouped[\"sem\"]\n",
    "        grouped[\"ci95\"] = grouped[\"ci95\"]\n",
    "\n",
    "        for context_value in [False, True]:\n",
    "            df_context = grouped[grouped[\"context\"] == context_value].copy()\n",
    "            context_str = \"context=True\" if context_value else \"context=False\"\n",
    "\n",
    "            fig = px.line(\n",
    "                df_context,\n",
    "                line_shape=\"spline\",\n",
    "                x=x_group,\n",
    "                template=\"plotly_white\",\n",
    "                y=\"mean\",\n",
    "                error_y=\"ci95\",\n",
    "                color=\"model_name\",\n",
    "                color_discrete_map=model_colors,\n",
    "                category_orders={\"model_name\": [\"Student\", \"qwen-student\", \"qwen-inst\", \"gpt-4.1\"]},\n",
    "                facet_col=\"prev_num\" if is_exp2 else None,\n",
    "                markers=True,\n",
    "                title=None,\n",
    "                labels={\"mean\": \"Avg. Style Score\", x_group: \"Submission Step\"},\n",
    "            )\n",
    "\n",
    "            for trace in fig.data:\n",
    "                trace.line.width = 2.5\n",
    "\n",
    "            fig.update_layout(\n",
    "                height=400,\n",
    "                width=1000,\n",
    "                font=dict(size=18, color='black'),\n",
    "                legend_title_text=None,\n",
    "                legend=dict(\n",
    "                    font=dict(size=22),\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.7,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5,\n",
    "                    bordercolor=\"grey\",\n",
    "                    borderwidth=1,\n",
    "                ),\n",
    "            )\n",
    "            fig.update_yaxes(matches=None)\n",
    "            fig.update_xaxes(tickmode=\"array\", tickvals=[2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "            fig.update_xaxes(\n",
    "                tickfont=dict(size=16, color='black'),\n",
    "                showgrid=True,\n",
    "                gridcolor='lightgray',\n",
    "                linecolor='black',\n",
    "                linewidth=1.0\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                tickfont=dict(size=16, color='black'),\n",
    "                showgrid=True,\n",
    "                gridcolor='lightgray',\n",
    "                linecolor='black',\n",
    "                linewidth=1.0\n",
    ")\n",
    "\n",
    "            print(f\"{question_name} — {test_class} — {context_str}\")\n",
    "            # fig.write_image(f\"plots/style/app/{question_name}_{test_class}_{context_str}_style.pdf\")\n",
    "            fig.show()\n",
    "\n",
    "\n",
    "# plot_avg_passrate_progress_multi(\n",
    "#     [(exp1_clean, \"exp1\")],\n",
    "#     x_col=\"quantile\",\n",
    "#     is_exp2=False,\n",
    "#     question_name=\"count_coins\",\n",
    "#     include_models=[\"qwen-student\", \"qwen-inst\", \"gpt-4.1\"]\n",
    "# )\n",
    "\n",
    "plot_avg_style_progress_multi(\n",
    "    [\n",
    "        (exp2_clean[exp2_clean[\"prev_num\"] == 1], \"exp2 prev=1\"),\n",
    "        (exp2_clean[exp2_clean[\"prev_num\"] == 3], \"exp2 prev=3\")\n",
    "    ],\n",
    "    x_col=\"norm_block_num\",\n",
    "    is_exp2=True,\n",
    "    question_name=\"two_list\",\n",
    "    include_models=[\"qwen-student\", \"qwen-inst\", \"gpt-4.1\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = ['model_name', 'context', 'prev_num', 'test_class', 'student_id', 'question_name']\n",
    "df = exp2_clean.sort_values(group_cols + ['block_num']).copy()\n",
    "\n",
    "df['gt_code_prev'] = df.groupby(group_cols)['gt_code_block'].shift(1)\n",
    "df['synthetic_code_prev'] = df.groupby(group_cols)['synthetic_code_block'].shift(1)\n",
    "\n",
    "df['gt_code_diff'] = df.apply(lambda row: Levenshtein.distance(str(row['gt_code_prev']), str(row['gt_code_block']))\n",
    "                              if pd.notnull(row['gt_code_prev']) else 0, axis=1)\n",
    "\n",
    "\n",
    "df['synthetic_code_diff'] = df.apply(lambda row: Levenshtein.distance(str(row['synthetic_code_prev']), str(row['synthetic_code_block']))\n",
    "                                     if pd.notnull(row['synthetic_code_prev']) else 0, axis=1)\n",
    "\n",
    "\n",
    "exp2_clean = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_diff_df(df, x_col, is_exp2=False):\n",
    "    df = df[df[\"context\"].notna()].copy()\n",
    "\n",
    "    if is_exp2 and \"prev_num\" in df.columns:\n",
    "        df[\"prev\"] = df[\"prev_num\"].astype(str)\n",
    "\n",
    "    gt_df = df.drop_duplicates(subset=[\"student_id\", \"question_name\", x_col]).copy()\n",
    "    gt_df[\"model_name\"] = \"Student\"\n",
    "    gt_df[\"code_diff\"] = gt_df[\"gt_code_diff\"]\n",
    "    gt_df[\"context\"] = False\n",
    "    gt_df2 = gt_df.copy()\n",
    "    gt_df2[\"context\"] = True\n",
    "    gt_df = pd.concat([gt_df, gt_df2], ignore_index=True)\n",
    "\n",
    "    syn_df = df.copy()\n",
    "    syn_df[\"code_diff\"] = syn_df[\"synthetic_code_diff\"]\n",
    "\n",
    "    combined = pd.concat([gt_df, syn_df], ignore_index=True)\n",
    "\n",
    "    if is_exp2:\n",
    "        combined[x_col] = combined[x_col].astype(int)\n",
    "    else:\n",
    "        if x_col == \"quantile\":\n",
    "            quantile_map = {\n",
    "                \"submission_q0\": \"start\",\n",
    "                \"submission_q1\": \"middle\",\n",
    "                \"submission_q2\": \"last\"\n",
    "            }\n",
    "            combined[x_col] = combined[x_col].map(quantile_map)\n",
    "            combined[x_col] = pd.Categorical(\n",
    "                combined[x_col],\n",
    "                categories=[\"start\", \"middle\", \"last\"],\n",
    "                ordered=True\n",
    "            )\n",
    "        else:\n",
    "            combined[x_col] = combined[x_col].astype(int)\n",
    "    combined[\"model_name\"] = combined[\"model_name\"].replace({\n",
    "        \"qwen_2_5_coder_7b\": \"qwen-student\",\n",
    "        \"qwen_2_5_coder_7b_inst\": \"qwen-inst\",\n",
    "        \"gpt_4_1\": \"gpt-4.1\",\n",
    "    })\n",
    "\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def plot_avg_diff_progress_multi(df_list, x_col, is_exp2=False, question_name=None, include_models=None):\n",
    "    combined_all = []\n",
    "    for df, label in df_list:\n",
    "        df_prepped = prepare_diff_df(df, x_col, is_exp2).copy()\n",
    "        if is_exp2 and \"prev_num\" in df.columns:\n",
    "            df_prepped[\"prev_num\"] = df_prepped[\"prev\"]\n",
    "        else:\n",
    "            df_prepped[\"prev_num\"] = \"All\"\n",
    "        combined_all.append(df_prepped)\n",
    "\n",
    "    combined_df = pd.concat(combined_all, ignore_index=True)\n",
    "\n",
    "    if question_name:\n",
    "        combined_df = combined_df[combined_df[\"question_name\"] == question_name]\n",
    "\n",
    "    if include_models is not None:\n",
    "        combined_df = combined_df[\n",
    "            (combined_df[\"model_name\"] == \"Student\")|\n",
    "            (combined_df[\"model_name\"].isin(include_models))\n",
    "        ]\n",
    "\n",
    "    for test_class in sorted(combined_df[\"test_class\"].dropna().unique()):\n",
    "        print(f\"== {question_name} — {test_class} — Avg Pass Rate Progress ==\")\n",
    "        df_tc = combined_df[combined_df[\"test_class\"] == test_class].copy()\n",
    "\n",
    "        if is_exp2:\n",
    "            bin_size = 1\n",
    "            df_tc[\"binned_step\"] = (df_tc[x_col] // bin_size) * bin_size\n",
    "            x_group = \"binned_step\"\n",
    "        else:\n",
    "            x_group = x_col\n",
    "\n",
    "        grouped = (\n",
    "            df_tc.groupby([x_group, \"model_name\", \"context\", \"prev_num\"])[\"code_diff\"]\n",
    "            .agg([\"mean\", \"std\", \"count\"])\n",
    "            .reset_index()\n",
    "        )\n",
    "        grouped[\"sem\"] = grouped[\"std\"] / grouped[\"count\"]**0.5\n",
    "        grouped[\"ci95\"] = 1.96 * grouped[\"sem\"]\n",
    "        grouped[\"ci95\"] = grouped[\"ci95\"]\n",
    "        print(grouped[[x_group, \"model_name\", \"mean\", \"ci95\", \"count\"]].head(10))\n",
    "\n",
    "\n",
    "        for context_value in [False, True]:\n",
    "            df_context = grouped[grouped[\"context\"] == context_value].copy()\n",
    "            context_str = \"context=True\" if context_value else \"context=False\"\n",
    "\n",
    "            fig = px.line(\n",
    "                df_context,\n",
    "                line_shape=\"spline\",\n",
    "                x=x_group,\n",
    "                template=\"plotly_white\",\n",
    "                y=\"mean\",\n",
    "                error_y=\"ci95\",\n",
    "                color=\"model_name\",\n",
    "                color_discrete_map=model_colors,\n",
    "                category_orders={\"model_name\": [\"Student\", \"qwen-student\", \"qwen-inst\", \"gpt-4.1\"]},\n",
    "                facet_col=\"prev_num\" if is_exp2 else None,\n",
    "                markers=True,\n",
    "                title=None,\n",
    "                labels={\"mean\": \"Avg. Code Edit Distance\", x_group: \"Submission Step\"},\n",
    "            )\n",
    "\n",
    "            for trace in fig.data:\n",
    "                trace.line.width = 2.5\n",
    "\n",
    "            fig.update_layout(\n",
    "                height=400,\n",
    "                width=1000,\n",
    "                font=dict(size=18, color='black'),\n",
    "                legend_title_text=None,\n",
    "                legend=dict(\n",
    "                    font=dict(size=22),\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.7,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5,\n",
    "                    bordercolor=\"grey\",\n",
    "                    borderwidth=1,\n",
    "                ),\n",
    "            )\n",
    "            fig.update_yaxes(matches=None)\n",
    "            fig.update_xaxes(tickmode=\"array\", tickvals=[2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "            fig.update_xaxes(\n",
    "                tickfont=dict(size=16, color='black'),\n",
    "                showgrid=True,\n",
    "                gridcolor='lightgray',\n",
    "                linecolor='black',\n",
    "                linewidth=1.0\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                tickfont=dict(size=16, color='black'),\n",
    "                showgrid=True,\n",
    "                gridcolor='lightgray',\n",
    "                linecolor='black',\n",
    "                linewidth=1.0\n",
    ")\n",
    "\n",
    "            print(f\"{question_name} — {test_class} — {context_str}\")\n",
    "            # fig.write_image(f\"plots/diff/app/{question_name}_{test_class}_{context_str}_diff.pdf\")\n",
    "            fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_avg_diff_progress_multi(\n",
    "    [\n",
    "        (exp2_clean[exp2_clean[\"prev_num\"] == 1], \"exp2 prev=1\"),\n",
    "        (exp2_clean[exp2_clean[\"prev_num\"] == 3], \"exp2 prev=3\")\n",
    "    ],\n",
    "    x_col=\"norm_block_num\",\n",
    "    is_exp2=True,\n",
    "    question_name=\"two_list\",\n",
    "    include_models=[\"qwen-student\", \"qwen-inst\", \"gpt-4.1\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_map = {\n",
    "    \"Student\": \"square\",\n",
    "    \"gpt_4_1\": \"cross\",\n",
    "    \"qwen_2_5_coder_7b_inst\": \"cross\",\n",
    "    \"qwen_2_5_coder_3b\": \"circle\",\n",
    "    \"qwen_2_5_coder_7b\": \"circle\",\n",
    "    \"qwen_3_8b\": \"circle\",\n",
    "    \"llama_3_8b\": \"circle\",\n",
    "    \"qwen-student\": \"circle\",\n",
    "    \"qwen-inst\": \"cross\",\n",
    "    \"gpt-4.1\": \"cross\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_distribution_per_testclass(\n",
    "    df,\n",
    "    title_prefix,\n",
    "    question_name=None,\n",
    "    include_models=None,\n",
    "    perplexity=30,\n",
    "    n_iter=500,\n",
    "    reducer=\"tsne\"\n",
    "):\n",
    "    df = df.copy()\n",
    "\n",
    "    \n",
    "    \n",
    "    if include_models is not None:\n",
    "        df = df[df[\"model_name\"].isin(include_models)]\n",
    "\n",
    "    if question_name:\n",
    "        df = df[df[\"question_name\"] == question_name]\n",
    "\n",
    "    for test_class in sorted(df[\"test_class\"].dropna().unique()):\n",
    "        all_tsne_rows = []\n",
    "\n",
    "        for quantile in sorted(df[\"quantile\"].dropna().unique()):\n",
    "            for context_val in [False, True]:\n",
    "                df_subset = df[\n",
    "                    (df[\"test_class\"] == test_class) &\n",
    "                    (df[\"quantile\"] == quantile) &\n",
    "                    (df[\"context\"] == context_val)\n",
    "                ].copy()\n",
    "\n",
    "                df_syn = df_subset[df_subset[\"model_name\"].notna()].copy()\n",
    "                df_syn[\"embedding\"] = df_syn[\"synthetic_code_block_embedding\"]\n",
    "                df_syn[\"source\"] = df_syn[\"model_name\"]\n",
    "                df_syn[\"quantile\"] = quantile\n",
    "                df_syn[\"test_class\"] = test_class\n",
    "\n",
    "                df_gt = df_subset.drop_duplicates(\n",
    "                    subset=[\"student_id\", \"question_name\", \"quantile\", \"context\", \"test_class\"]\n",
    "                ).copy()\n",
    "                df_gt[\"embedding\"] = df_gt[\"gt_code_block_embedding\"]\n",
    "                df_gt[\"source\"] = \"GT\"\n",
    "                df_gt[\"quantile\"] = quantile\n",
    "                df_gt[\"test_class\"] = test_class\n",
    "\n",
    "                combined = pd.concat([df_syn, df_gt], ignore_index=True)\n",
    "                combined[\"source\"] = combined[\"source\"].replace({\"GT\": \"Student\"})\n",
    "\n",
    "                valid_combined = combined[combined[\"embedding\"].apply(lambda x: isinstance(x, (list, np.ndarray)) and len(x) > 0)]\n",
    "                if len(valid_combined) < 2:\n",
    "                    continue\n",
    "\n",
    "                embeddings = np.array(valid_combined[\"embedding\"].tolist())\n",
    "                if reducer == \"tsne\":\n",
    "                    coords = TSNE(\n",
    "                        n_components=2,\n",
    "                        perplexity=min(perplexity, len(valid_combined) - 1),\n",
    "                        n_iter=n_iter,\n",
    "                        random_state=42\n",
    "                    ).fit_transform(embeddings)\n",
    "                elif reducer == \"umap\":\n",
    "                    coords = UMAP(\n",
    "                        n_components=2,\n",
    "                        random_state=42,\n",
    "                        min_dist=0.7,         \n",
    "                        n_neighbors=5,        \n",
    "                        spread=2.0,         \n",
    "                        metric='cosine',\n",
    "                        init='random',\n",
    "                        set_op_mix_ratio=0.1  \n",
    "                    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "                elif reducer == \"pca\":\n",
    "                    coords = PCA(n_components=2).fit_transform(embeddings)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown reducer: {reducer}\")\n",
    "\n",
    "                valid_combined[\"PC 1\"] = coords[:, 0]\n",
    "                valid_combined[\"PC 2\"] = coords[:, 1]\n",
    "\n",
    "                jitter_strength = 0.5\n",
    "                valid_combined[\"PC 1\"] += np.random.normal(0, jitter_strength, size=len(valid_combined))\n",
    "                valid_combined[\"PC 2\"] += np.random.normal(0, jitter_strength, size=len(valid_combined))\n",
    "\n",
    "                all_tsne_rows.append(valid_combined)\n",
    "\n",
    "\n",
    "        if not all_tsne_rows:\n",
    "            continue\n",
    "\n",
    "        final_df = pd.concat(all_tsne_rows, ignore_index=True)\n",
    "\n",
    "        quantile_map = {\n",
    "            \"submission_q0\": \"first\",\n",
    "            \"submission_q1\": \"middle\",\n",
    "            \"submission_q2\": \"last\"\n",
    "        }\n",
    "        final_df[\"quantile\"] = final_df[\"quantile\"].map(quantile_map)\n",
    "        final_df[\"quantile\"] = pd.Categorical(final_df[\"quantile\"], categories=[\"first\", \"middle\", \"last\"], ordered=True)\n",
    "\n",
    "        final_df = final_df.sort_values(\"quantile\")\n",
    "\n",
    "\n",
    "        counts = final_df.groupby([\"quantile\", \"context\", \"source\", \"test_class\"]).size().reset_index(name=\"count\")\n",
    "        print(f\"\\nData point counts for test_class = {test_class}:\\n\", counts)\n",
    "\n",
    "        final_df[\"source_order\"] = final_df[\"source\"].apply(lambda s: 0 if s == \"Student\" else 1)\n",
    "        final_df = final_df.sort_values(\"source_order\").drop(columns=\"source_order\")\n",
    "        \n",
    "        final_df[\"context_label\"] = final_df[\"context\"].map({False: \"context=False\", True: \"context=True\"})\n",
    "        final_df[\"context_label\"] = pd.Categorical(final_df[\"context_label\"], categories=[\"context=False\", \"context=True\"], ordered=True)\n",
    "\n",
    "        final_df[\"source\"] = final_df[\"source\"].replace({\n",
    "            \"qwen_2_5_coder_7b\": \"qwen-student\",\n",
    "            \"qwen_2_5_coder_7b_inst\": \"qwen-inst\",\n",
    "            \"gpt_4_1\": \"gpt-4.1\",\n",
    "            \"Student\": \"Student\"\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            final_df,\n",
    "            template=\"plotly_white\",\n",
    "            x=\"PC 1\",\n",
    "            y=\"PC 2\",\n",
    "            color=\"source\",\n",
    "            symbol=\"source\",\n",
    "            facet_row=\"context_label\",\n",
    "            facet_col=\"quantile\",\n",
    "            category_orders={\n",
    "                \"quantile\": [\"first\", \"middle\", \"last\"],\n",
    "                \"context_label\": [\"context=False\", \"context=True\"],\n",
    "                \"source\": [\"Student\", \"qwen-student\", \"qwen-inst\", \"gpt-4.1\"]\n",
    "            },\n",
    "            hover_name=\"student_id\",\n",
    "\n",
    "            color_discrete_map=model_colors,\n",
    "            title=None\n",
    "\n",
    "        )\n",
    "\n",
    "        fig.update_layout(height=650, width=1000)\n",
    "        fig.update_xaxes(showgrid=False, zeroline=False)\n",
    "        fig.update_yaxes(showgrid=False, zeroline=False)\n",
    "\n",
    "        fig.update_traces(marker=dict(line=dict(width=0.5, color=\"white\")))\n",
    "        fig.for_each_trace(lambda t: t.update(marker_symbol=symbol_map.get(t.name, \"circle\")))\n",
    "        fig.for_each_trace(lambda t: t.update(marker=dict(size=5 if t.name == \"Student\" else 6.5)))\n",
    "\n",
    "        fig.for_each_annotation(lambda a: a.update(text=a.text.replace(\"quantile=\", \"\").replace(\"context_label=\", \"\")))\n",
    "        fig.update_layout(margin=dict(l=10, r=10, t=80, b=10))\n",
    "        fig.update_layout(plot_bgcolor='white', paper_bgcolor='white')\n",
    "        fig.for_each_annotation(lambda a: a.update(font=dict(size=22))) \n",
    "\n",
    "        fig.update_xaxes(\n",
    "            showline=True,\n",
    "            linecolor='black',\n",
    "            linewidth=1,\n",
    "            tickfont=dict(size=16, color='black')\n",
    "        )\n",
    "\n",
    "        fig.update_yaxes(\n",
    "            showline=True,\n",
    "            linecolor='black',\n",
    "            linewidth=1,\n",
    "            tickfont=dict(size=16, color='black')\n",
    "        )\n",
    "\n",
    "\n",
    "        fig.update_xaxes(showticklabels=False)\n",
    "        fig.update_yaxes(showticklabels=False)\n",
    " \n",
    "        fig.update_layout(\n",
    "            margin=dict(l=10, r=10, t=30, b=10), \n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "            font=dict(size=17, color=\"black\"),\n",
    "            legend_title_text=None,\n",
    "            legend=dict(\n",
    "                font=dict(size=22),\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-0.25,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5,\n",
    "                bordercolor=\"grey\",\n",
    "                borderwidth=1,\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                itemsizing='constant',\n",
    "                font=dict(size=24),\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-0.25,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5,\n",
    "                bordercolor=\"grey\",\n",
    "                borderwidth=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        for axis in fig.layout:\n",
    "            if axis.startswith(\"xaxis\") and \"context=False\" in str(fig.layout[axis]):\n",
    "                fig.layout[axis].update(showticklabels=True)\n",
    "            if axis.startswith(\"yaxis\") and \"Bin 1\" in str(fig.layout[axis]):\n",
    "                fig.layout[axis].update(showticklabels=True)\n",
    "                fig.update_xaxes(title_text=\"PC 1\")\n",
    "                fig.update_yaxes(title_text=\"PC 2\")\n",
    "         \n",
    "        print(f\"{title_prefix} — {question_name} — {test_class} — Code Embeddings\")\n",
    "        # fig.write_image(f\"plots/embd/app/{question_name}_{test_class}_embd.pdf\")\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_tsne_distribution_per_testclass(\n",
    "    exp1_clean,\n",
    "    title_prefix=\"exp1\",\n",
    "    question_name=\"two_list\",\n",
    "    include_models=[\"qwen_2_5_coder_7b\", \"qwen_2_5_coder_7b_inst\", \"gpt_4_1\"],\n",
    "    reducer=\"pca\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_metrics(df, question_name=None, include_models=None, k_distance=3, k_coverage=3):\n",
    "    results_by_test_class = {}\n",
    "\n",
    "    if question_name:\n",
    "        df = df[df[\"question_name\"] == question_name]\n",
    "    if include_models:\n",
    "        df = df[df[\"model_name\"].isin(include_models)]\n",
    "\n",
    "    contexts = df[\"context\"].dropna().unique()\n",
    "    quantiles = df[\"quantile\"].dropna().unique()\n",
    "    test_classes = df[\"test_class\"].dropna().unique()\n",
    "    model_names = df[\"model_name\"].dropna().unique()\n",
    "\n",
    "    for test_class in test_classes:\n",
    "        df_tc = df[df[\"test_class\"] == test_class]\n",
    "        results = []\n",
    "\n",
    "        for context in contexts:\n",
    "            for quantile in quantiles:\n",
    "                for model in model_names:\n",
    "                    student_emb = df_tc[\n",
    "                        (df_tc[\"model_name\"]==model) &\n",
    "                        (df_tc[\"context\"]==context) &\n",
    "                        (df_tc[\"quantile\"]==quantile)\n",
    "                    ][\"gt_code_block_embedding\"].tolist()\n",
    "                    if not student_emb:\n",
    "                        continue\n",
    "                    student_emb = np.vstack(student_emb)\n",
    "                    \n",
    "                    model_emb = df_tc[\n",
    "                        (df_tc[\"model_name\"]==model) &\n",
    "                        (df_tc[\"context\"]==context) &\n",
    "                        (df_tc[\"quantile\"]==quantile)\n",
    "                    ][\"synthetic_code_block_embedding\"].tolist()\n",
    "                    if not model_emb:\n",
    "                        continue\n",
    "                    model_emb = np.vstack(model_emb)\n",
    "                    \n",
    "                    dist_student_model = 1.0 - cos_sim(student_emb, model_emb).numpy()\n",
    "                    dist_model_student = 1.0 - cos_sim(model_emb, student_emb).numpy()\n",
    "                    \n",
    "                    # NN distance\n",
    "                    nn_dist = np.sort(dist_student_model, axis=1)[:, :k_distance].mean()\n",
    "                    \n",
    "                    # NN coverage\n",
    "                    nn_idx = np.argsort(dist_model_student, axis=1)[:, :k_coverage]\n",
    "                    covered = np.unique(nn_idx).size\n",
    "                    coverage = covered / student_emb.shape[0]\n",
    "\n",
    "                    results.append({\n",
    "                        \"question\": df_tc[\"question_name\"].iloc[0],\n",
    "                        \"test_class\": test_class,\n",
    "                        \"model\": model,\n",
    "                        \"quantile\": quantile,\n",
    "                        \"context\": context,\n",
    "                        \"avg_knn_dist\": round(nn_dist, 4),\n",
    "                        \"knn_coverage\": round(coverage, 4)\n",
    "                    })\n",
    "\n",
    "        results_by_test_class[test_class] = pd.DataFrame(results).sort_values(\n",
    "            [\"model\", \"quantile\", \"context\"]\n",
    "        )\n",
    "\n",
    "    return results_by_test_class\n",
    "\n",
    "\n",
    "tables = compute_embedding_metrics(\n",
    "    exp1_clean,\n",
    "    question_name=\"two_list\",\n",
    "    include_models=[\"qwen_2_5_coder_7b\", \"qwen_2_5_coder_7b_inst\", \"gpt_4_1\", \"qwen_3_8b\", \"llama_3_8b\", \"qwen_2_5_coder_3b\"],\n",
    "    # include_models=[\"qwen_3_8b\", \"llama_3_8b\", \"qwen_2_5_coder_3b\"],\n",
    "\n",
    "    k_distance=3,\n",
    "    k_coverage=10\n",
    ")\n",
    "for test_class, df in tables.items():\n",
    "    print(f\"\\n=== Test Class: {test_class} ===\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_loss = [\n",
    "    \"loc\", \"char_count\", \"ast_depth\", \"ast_width\",\n",
    "    \"ast_node_count\", \n",
    "    \"pep8_violations.count\", \"style_score\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_and_embedding_errors(df):\n",
    "    for key in keys_loss:\n",
    "        df[f\"{key}_abs_error\"] = (\n",
    "            df[f\"gt_{key}\"] - df[f\"synthetic_{key}\"]\n",
    "        ).abs()\n",
    "\n",
    "    df[\"test_pass_rate_abs_error\"] = (\n",
    "        df[\"gt_test_pass_rate\"] - df[\"synthetic_test_pass_rate\"]\n",
    "    ).abs()\n",
    "\n",
    "    if \"gt_style_score\" in df.columns and \"synthetic_style_score\" in df.columns:\n",
    "        df[\"style_score_abs_error\"] = (\n",
    "            df[\"gt_style_score\"] - df[\"synthetic_style_score\"]\n",
    "        ).abs()\n",
    "\n",
    "    def compute_cosine_dist(row):\n",
    "        if isinstance(row[\"gt_code_block_embedding\"], list) and isinstance(row[\"synthetic_code_block_embedding\"], list):\n",
    "            return cosine_distances(\n",
    "                [row[\"gt_code_block_embedding\"]],\n",
    "                [row[\"synthetic_code_block_embedding\"]]\n",
    "            )[0][0]\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"embedding_cosine_distance\"] = df.apply(compute_cosine_dist, axis=1)\n",
    "\n",
    "    df[\"error_type_match\"] = (\n",
    "        df[\"gt_error_type\"] == df[\"synthetic_error_type\"]\n",
    "    ).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "exp1_clean = compute_style_and_embedding_errors(exp1_clean)\n",
    "exp2_clean = compute_style_and_embedding_errors(exp2_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_errors(\n",
    "    df,\n",
    "    question_name=None,\n",
    "    test_class=None,\n",
    "    include_prev_num=False \n",
    "):\n",
    "    filtered = df.copy()\n",
    "    if question_name:\n",
    "        filtered = filtered[filtered[\"question_name\"] == question_name]\n",
    "    if test_class:\n",
    "        filtered = filtered[filtered[\"test_class\"] == test_class]\n",
    "\n",
    "    group_keys = [\"model_name\", \"quantile\", \"context\"]\n",
    "\n",
    "    error_cols = [col for col in filtered.columns if (\n",
    "        col.endswith(\"_abs_error\")\n",
    "        or col.endswith(\"_distance\")\n",
    "        or col == \"error_type_match\"\n",
    "    )]\n",
    "\n",
    "    summary = (\n",
    "        filtered[group_keys + error_cols]\n",
    "        .groupby(group_keys)\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    allowed_models = [\"gpt_4_1\", \"qwen_2_5_coder_7b\", \"qwen_2_5_coder_7b_inst\", \"GT\"]\n",
    "    # allowed_models=[\"qwen_3_8b\", \"llama_3_8b\", \"qwen_2_5_coder_3b\",  \"GT\"]\n",
    "\n",
    "    summary = summary[summary[\"model_name\"].isin(allowed_models)]\n",
    "\n",
    "    selected_cols = [\"pep8_violations.count_abs_error\", \"style_score_abs_error\"]\n",
    "    summary = summary[group_keys + selected_cols]\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "summarize_errors(exp1_clean, question_name=\"two_list\", test_class=\"test1\")\n",
    "# summarize_errors(exp2_clean, question_name=\"count_coins\", test_class=\"test3\", include_prev_num=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
